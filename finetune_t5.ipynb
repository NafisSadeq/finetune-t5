{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af59377-0028-495f-b924-acc0f4d4f910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/nafis/.virtualenvs/crs/lib/python3.10/site-packages (0.1.97)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy scikit-learn pandas torch transformers sentencepiece tqdm wandb jupyterlab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65fb1ef-b345-42cb-9e05-a19b7e63a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b469d27-35fd-4109-8e35-80334dbfdded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_name, history_length, tokenizer):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.text,self.ctext,self.source_len,self.summ_len = self._load_data_(file_name,history_length)\n",
    "        \n",
    "    def _load_data_(self,file_name,history_length):\n",
    "        \n",
    "        train_dialogue=[]\n",
    "        train_utt_source=[]\n",
    "        train_utt_target=[]\n",
    "        history_len=5\n",
    "        \n",
    "        with open(file_name,'r') as file:\n",
    "            for line in file:\n",
    "                train_dialogue.append(eval(line.strip().replace('null',\"None\")))\n",
    "\n",
    "        for index in range(len(train_dialogue)):\n",
    "            if type(train_dialogue[index]['movieMentions']) == list:\n",
    "                continue\n",
    "            movie_key_list=train_dialogue[index]['movieMentions'].keys()\n",
    "            dialog_history=[]\n",
    "            for utt in train_dialogue[index][\"messages\"]:\n",
    "                source_text=[]\n",
    "                target_text=utt[\"text\"]\n",
    "                flag=False\n",
    "                for key in movie_key_list:\n",
    "                    if(\"@\"+key in utt[\"text\"]):\n",
    "                        flag=True\n",
    "                        target_text=target_text.replace(\"@\"+key,train_dialogue[index]['movieMentions'][key])\n",
    "                        source_text.append(train_dialogue[index]['movieMentions'][key])\n",
    "                if(flag):\n",
    "                    source_text=\" \".join(source_text)+\": \"+\" \".join(dialog_history[-history_len:])\n",
    "                    train_utt_source.append(source_text)\n",
    "                    train_utt_target.append(target_text)\n",
    "                dialog_history.append(target_text)\n",
    "                \n",
    "        max_source_len=0\n",
    "        max_target_len=0\n",
    "\n",
    "        for i in range(len(train_utt_source)):\n",
    "            max_source_len=max(max_source_len,len(train_utt_source[i].split()))\n",
    "            max_target_len=max(max_target_len,len(train_utt_target[i].split()))\n",
    "            \n",
    "        return train_utt_source,train_utt_target,max_source_len,max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitemtext__(self, index):\n",
    "        return self.text[index],self.ctext[index]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([text], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([ctext], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbdebbe-25de-4c12-9a2e-e318446d3916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nafis/.virtualenvs/crs/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer=T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "train_dataset=CustomDataset(\"train_data.jsonl\",5,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fdc9fc-80a0-4a5f-bc9f-0a5b821ae273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Perfect Storm  (2000): Hi What kind of movies do you like? Hey there I would love to suggest some movies for you, what genre do you prefer?',\n",
       " \"I'm looking for something like The Perfect Storm  (2000)\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitemtext__(45002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9abfdf-39a4-4488-afc6-0a9819c27056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(tqdm(loader), 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%10 == 0:\n",
    "            wandb.log({\"Training Loss\": loss.item()})\n",
    "\n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9798091-d58b-4c71-9162-5d9e84b8d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _,data in enumerate(tqdm(loader), 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            if _%100==0:\n",
    "                print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b1debf-5e77-40a7-9a11-a686b645ec04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnsadeq\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/nafis/Data/Projects/finetune-t5/wandb/run-20230223_185725-3hgr59a8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nsadeq/t5%20baseline%20conversational%20recsys/runs/3hgr59a8\" target=\"_blank\">proud-spaceship-19</a></strong> to <a href=\"https://wandb.ai/nsadeq/t5%20baseline%20conversational%20recsys\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nafis/.virtualenvs/crs/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Fine-Tuning for the model on our dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e63c507c8d423fb576ea8a0cc62a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/nafis/.virtualenvs/crs/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  8.68327808380127\n",
      "Epoch: 0, Loss:  1.4208472967147827\n",
      "Epoch: 0, Loss:  1.2840797901153564\n",
      "Epoch: 0, Loss:  1.5103020668029785\n",
      "Epoch: 0, Loss:  1.8174607753753662\n",
      "Epoch: 0, Loss:  2.1884849071502686\n",
      "Epoch: 0, Loss:  2.29805850982666\n",
      "Epoch: 0, Loss:  1.3372281789779663\n",
      "Epoch: 0, Loss:  1.778540849685669\n",
      "Epoch: 0, Loss:  0.9215937256813049\n",
      "Epoch: 0, Loss:  0.9351845383644104\n",
      "Epoch: 0, Loss:  1.2724143266677856\n",
      "Epoch: 0, Loss:  1.7890400886535645\n",
      "Epoch: 0, Loss:  1.135810136795044\n",
      "Epoch: 0, Loss:  1.468592882156372\n",
      "Now generating responses on our fine tuned model for the validation dataset and saving it in a dataframe\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93de01057854592a30e867f82ea54bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/891 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0\n",
      "Completed 100\n",
      "Completed 200\n",
      "Completed 300\n",
      "Completed 400\n",
      "Completed 500\n",
      "Completed 600\n",
      "Completed 700\n",
      "Completed 800\n",
      "Output Files generated for review\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"t5 baseline conversational recsys\")\n",
    "\n",
    "# WandB â€“ Config is a variable that holds and saves hyperparameters and inputs\n",
    "# Defining some key variables that will be used later on in the training  \n",
    "config = wandb.config          # Initialize config\n",
    "config.TRAIN_BATCH_SIZE = 8    # input batch size for training (default: 64)\n",
    "config.VALID_BATCH_SIZE = 8    # input batch size for testing (default: 1000)\n",
    "config.TRAIN_EPOCHS = 1        # number of epochs to train (default: 10)\n",
    "config.VAL_EPOCHS = 1 \n",
    "config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "config.SEED = 42               # random seed (default: 42)\n",
    "config.MAX_LEN = 512\n",
    "config.SUMMARY_LEN = 150 \n",
    "\n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(config.SEED) # pytorch random seed\n",
    "np.random.seed(config.SEED) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# tokenzier for encoding the text\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "train_dataset=CustomDataset(\"train_data.jsonl\",5,tokenizer)\n",
    "val_dataset=CustomDataset(\"test_data.jsonl\",5,tokenizer)\n",
    "\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "    'batch_size': config.TRAIN_BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "val_params = {\n",
    "    'batch_size': config.VALID_BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(train_dataset, **train_params)\n",
    "val_loader = DataLoader(val_dataset, **val_params)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "# Log metrics with wandb\n",
    "wandb.watch(model, log=\"all\")\n",
    "# Training loop\n",
    "print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "for epoch in range(config.TRAIN_EPOCHS):\n",
    "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "\n",
    "# Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "# Saving the dataframe as predictions.csv\n",
    "print('Now generating responses on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "for epoch in range(config.VAL_EPOCHS):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    final_df.to_csv('predictions.tsv',sep=\"\\t\")\n",
    "    print('Output Files generated for review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e848ad9-ae07-4961-9dce-83c84af312fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Last House on the Left  (1972): Hello! What kind of movies do you like? I am looking for a movie recommendation.   When I was younger I really enjoyed the A Nightmare on Elm Street (1984) Oh, you like scary movies? I recently watched Happy Death Day  (2017)',\n",
       " 'I also enjoyed watching The Last House on the Left  (1972)')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.__getitemtext__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3d82e5b-0292-4af5-9fc0-2910d84bbb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (success).\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"models/t5_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53087a8-1ed7-45a4-89c8-27a4d9bbf40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
