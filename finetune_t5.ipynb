{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af59377-0028-495f-b924-acc0f4d4f910",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fb1ef-b345-42cb-9e05-a19b7e63a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b469d27-35fd-4109-8e35-80334dbfdded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_name, history_length, tokenizer):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.text,self.ctext,self.source_len,self.summ_len = self._load_data_(file_name,history_length)\n",
    "        \n",
    "    def _load_data_(self,file_name,history_length):\n",
    "        \n",
    "        train_dialogue=[]\n",
    "        train_utt_source=[]\n",
    "        train_utt_target=[]\n",
    "        history_len=5\n",
    "        \n",
    "        with open(file_name,'r') as file:\n",
    "            for line in file:\n",
    "                train_dialogue.append(eval(line.strip().replace('null',\"None\")))\n",
    "\n",
    "        for index in range(len(train_dialogue)):\n",
    "            if type(train_dialogue[index]['movieMentions']) == list:\n",
    "                continue\n",
    "            movie_key_list=train_dialogue[index]['movieMentions'].keys()\n",
    "            dialog_history=[]\n",
    "            for utt in train_dialogue[index][\"messages\"]:\n",
    "                source_text=[]\n",
    "                target_text=utt[\"text\"]\n",
    "                flag=False\n",
    "                for key in movie_key_list:\n",
    "                    if(\"@\"+key in utt[\"text\"]):\n",
    "                        flag=True\n",
    "                        target_text=target_text.replace(\"@\"+key,train_dialogue[index]['movieMentions'][key])\n",
    "                        source_text.append(train_dialogue[index]['movieMentions'][key])\n",
    "                if(flag):\n",
    "                    source_text=\" \".join(source_text)+\": \"+\" \".join(dialog_history[-history_len:])\n",
    "                    train_utt_source.append(source_text)\n",
    "                    train_utt_target.append(target_text)\n",
    "                dialog_history.append(target_text)\n",
    "                \n",
    "        max_source_len=0\n",
    "        max_target_len=0\n",
    "\n",
    "        for i in range(len(train_utt_source)):\n",
    "            max_source_len=max(max_source_len,len(train_utt_source[i].split()))\n",
    "            max_target_len=max(max_target_len,len(train_utt_target[i].split()))\n",
    "            \n",
    "        return train_utt_source,train_utt_target,max_source_len,max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitemtext__(self, index):\n",
    "        return self.text[index],self.ctext[index]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([text], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([ctext], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbdebbe-25de-4c12-9a2e-e318446d3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "train_dataset=CustomDataset(\"train_data.jsonl\",5,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fdc9fc-80a0-4a5f-bc9f-0a5b821ae273",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__getitemtext__(45002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9abfdf-39a4-4488-afc6-0a9819c27056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(tqdm(loader), 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%10 == 0:\n",
    "            wandb.log({\"Training Loss\": loss.item()})\n",
    "\n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9798091-d58b-4c71-9162-5d9e84b8d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _,data in enumerate(tqdm(loader), 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            if _%100==0:\n",
    "                print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1debf-5e77-40a7-9a11-a686b645ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"t5 baseline conversational recsys\")\n",
    "\n",
    "# WandB â€“ Config is a variable that holds and saves hyperparameters and inputs\n",
    "# Defining some key variables that will be used later on in the training  \n",
    "config = wandb.config          # Initialize config\n",
    "config.TRAIN_BATCH_SIZE = 8    # input batch size for training (default: 64)\n",
    "config.VALID_BATCH_SIZE = 8    # input batch size for testing (default: 1000)\n",
    "config.TRAIN_EPOCHS = 1        # number of epochs to train (default: 10)\n",
    "config.VAL_EPOCHS = 1 \n",
    "config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "config.SEED = 42               # random seed (default: 42)\n",
    "config.MAX_LEN = 512\n",
    "config.SUMMARY_LEN = 150 \n",
    "\n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(config.SEED) # pytorch random seed\n",
    "np.random.seed(config.SEED) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# tokenzier for encoding the text\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "train_dataset=CustomDataset(\"train_data.jsonl\",5,tokenizer)\n",
    "val_dataset=CustomDataset(\"test_data.jsonl\",5,tokenizer)\n",
    "\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "    'batch_size': config.TRAIN_BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "val_params = {\n",
    "    'batch_size': config.VALID_BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(train_dataset, **train_params)\n",
    "val_loader = DataLoader(val_dataset, **val_params)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "# Log metrics with wandb\n",
    "wandb.watch(model, log=\"all\")\n",
    "# Training loop\n",
    "print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "for epoch in range(config.TRAIN_EPOCHS):\n",
    "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "\n",
    "# Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "# Saving the dataframe as predictions.csv\n",
    "print('Now generating responses on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "for epoch in range(config.VAL_EPOCHS):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    final_df.to_csv('predictions.csv')\n",
    "    print('Output Files generated for review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e848ad9-ae07-4961-9dce-83c84af312fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.__getitemtext__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d82e5b-0292-4af5-9fc0-2910d84bbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/t5_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53087a8-1ed7-45a4-89c8-27a4d9bbf40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
